---
title: "Statistical Inference - Logistic Regression"
author: "Ian Dinwoodie"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(lme4)
library(psych)
library(homals)
library(e1071)

knitr::opts_chunk$set(echo=TRUE)
set.seed(1)
```

# Preparing the Data

## Loading the Tidy Data

Load the tidy data from disk.

```{r}
df <- readRDS('../data/processed/tidy.Rds')
#stopifnot(identical(dim(df)+0, c(1543, 28)))
str(df, list.len=5)
```

## Data Preperation

Collapse entries by owner.

```{r}
rank_cols = c("age_rank", "appearance_rank", "breed_rank", "compatability_rank",
              "personality_rank", "size_rank", "trainability_rank")
df %>%
  mutate(is_satisfied=ifelse(is_satisfied, 1, 0)) %>%
  group_by(owner_id) %>%
  mutate_at(rank_cols, ~ mean(.)) %>%
  
```

Drop columns that won't be used by the inferential models.

```{r}
df <- df %>%
  dplyr::select(-c(
    owner_id,
    dog_name,
    time_together_len,
    revised_acquisition_source,
    #is_living_with_dog,
    curr_dog_location,
    met_expectations,
    is_consider_another_dog
  )) %>%
  dplyr::select(-contains("revised"))

dim(df)
summary(df)
```

Remove problematic variables and rows with missing values.

```{r}
df <- df %>%
  #dplyr::select(-trainability_rank) %>%
  #dplyr::select(-is_male) %>%
  mutate(is_satisfied = as.factor(is_satisfied)) %>%
  #mutate(is_satisfied = as.factor(ifelse(is_satisfied, "yes", "no"))) %>%
  drop_na()
```

# Inference

```{r}
features = df[,6:12]
## Pearson Correlation
pear_cor = cor(features)
cor.plot(pear_cor, numbers=T, upper=FALSE, main = "Pearson Correlation", show.legend = FALSE)
```
```{r}
## Polychoric correlation
poly_cor = polychoric(features)
rho = poly_cor$rho
save(rho, file = "polychoric")
### Thresholds/Scaling results
poly_cor$tau
```

```{r}
cor.plot(poly_cor$rho, numbers=T, upper=FALSE, main = "Polychoric Correlation", show.legend = FALSE)
```

```{r}
set.seed(1)
poly_model = fa(features, nfactor=3, cor="poly", fm="mle", rotate = "none")
poly_model$loadings
```

```{r}
fa.diagram(poly_model)
```

```{r}
biplot(
  poly_model,
  pch=c(16, 17),
  group=(ifelse(df$is_satisfied==T, 1, 2)),
  main="Biplot by Expectations",
  col = c("orange","blue")
)
```

```{r}
naiveBayes(is_satisfied ~., data=df[,6:13])
```

## Preprocessing

```{r}
mm <- model.matrix(is_satisfied~.-1, df)
str(mm)
```

```{r}
# Eliminate near zero-variance variables.
(nzv_metrics <- nearZeroVar(mm, saveMetrics=TRUE))
nzv_cnt <- sum(nzv_metrics$nzv)
print(paste("Near zero-variance vars to eliminate:", nzv_cnt))
if (nzv_cnt) {
  print(colnames(mm[, nzv_metrics$nzv]))
  mm <- mm[, !nzv_metrics$nzv]
}

# Eliminate linearly correlated predictors.
combo_info <- findLinearCombos(mm)
combo_cnt <- length(combo_info$remove)
print(paste("Linearly corr. combos vars to eliminate:", combo_cnt))
if (combo_cnt) {
  print(colnames(mm[, combo_info$remove]))
  mm <- mm[, -combo_info$remove]
}
```

```{r}
df <- cbind.data.frame(is_satisfied=df$is_satisfied, mm)
colnames(df) <- make.names(colnames(df))
dim(df)
summary(df)
```

# Save Session Info

```{r}
sessionInfo()
```
